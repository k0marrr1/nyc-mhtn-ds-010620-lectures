{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=0.38245219347581555,\n",
       "              seed=None, silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793722\n",
      "F1: 0.676056\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639739</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.616502</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.639739           0.006761           0.640184   \n",
       "2              0.627790           0.006070           0.629019   \n",
       "3              0.614868           0.010608           0.616502   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06172185, 0.17381749, 0.28124804, 0.06234765, 0.1694633 ,\n",
       "        0.27716427, 0.06045294, 0.16956873, 0.28471684, 0.06982603,\n",
       "        0.21154485, 0.33152089, 0.0722208 , 0.19498482, 0.36632767,\n",
       "        0.08718252, 0.20541   , 0.32525997, 0.08197746, 0.22411542,\n",
       "        0.37060618, 0.07721705, 0.22433944, 0.36657047, 0.0769546 ,\n",
       "        0.21647105, 0.35361037, 0.08491559, 0.24981732, 0.39876714,\n",
       "        0.08585501, 0.23551521, 0.39567914, 0.08349233, 0.23916211,\n",
       "        0.38284416, 0.09232898, 0.26314664, 0.4362422 , 0.09578919,\n",
       "        0.25169091, 0.40994244, 0.08915539, 0.24800043, 0.41116915,\n",
       "        0.06483932, 0.16854982, 0.28241568, 0.06414847, 0.18359447,\n",
       "        0.27998018, 0.06171293, 0.16952553, 0.28973174, 0.0754055 ,\n",
       "        0.20233059, 0.32852459, 0.07312303, 0.19457011, 0.32161827,\n",
       "        0.07174301, 0.20392871, 0.33187046, 0.07774887, 0.23272319,\n",
       "        0.37369781, 0.07626758, 0.21800623, 0.36984968, 0.07859526,\n",
       "        0.21589842, 0.35179062, 0.08896713, 0.24973469, 0.39845295,\n",
       "        0.08693104, 0.24588814, 0.40047302, 0.08474956, 0.23127146,\n",
       "        0.39172134, 0.09361243, 0.26237125, 0.43649564, 0.09467363,\n",
       "        0.26120386, 0.42146573, 0.099753  , 0.25379543, 0.42074509,\n",
       "        0.06207342, 0.16924953, 0.27698741, 0.06156077, 0.1770123 ,\n",
       "        0.28463049, 0.06023073, 0.1725636 , 0.28534017, 0.07105646,\n",
       "        0.21269064, 0.33116755, 0.07289934, 0.20134659, 0.32880554,\n",
       "        0.07004304, 0.20207133, 0.32214918, 0.07820134, 0.22065125,\n",
       "        0.36471963, 0.07625442, 0.21796188, 0.36084542, 0.07830071,\n",
       "        0.22004557, 0.36405287, 0.08860712, 0.25147529, 0.42032137,\n",
       "        0.08694153, 0.24782872, 0.40241423, 0.09128661, 0.24236584,\n",
       "        0.39931641, 0.09546385, 0.27130718, 0.45892997, 0.09533944,\n",
       "        0.26800318, 0.43701086, 0.08983107, 0.25573177, 0.42611046,\n",
       "        0.06480608, 0.17680545, 0.2935358 , 0.06319885, 0.16985316,\n",
       "        0.28913484, 0.06224647, 0.1780664 , 0.29211221, 0.07298927,\n",
       "        0.20596085, 0.33412418, 0.07148876, 0.22179608, 0.33210254,\n",
       "        0.07146125, 0.19986701, 0.335783  , 0.08409014, 0.22650447,\n",
       "        0.44843216, 0.09241276, 0.22588215, 0.37889142, 0.08650222,\n",
       "        0.23402801, 0.36834655, 0.08958201, 0.2507319 , 0.41782122,\n",
       "        0.08735085, 0.2426127 , 0.41126666, 0.09172282, 0.242729  ,\n",
       "        0.39847975, 0.09468799, 0.27533178, 0.44732518, 0.0927546 ,\n",
       "        0.26476202, 0.43984823, 0.09176388, 0.26180902, 0.42968102,\n",
       "        0.06440725, 0.17427001, 0.29642019, 0.06635966, 0.18633771,\n",
       "        0.29051943, 0.06230903, 0.17712221, 0.2962091 , 0.07493353,\n",
       "        0.20902491, 0.33322477, 0.07166181, 0.20471025, 0.3347199 ,\n",
       "        0.07765045, 0.20420704, 0.34379888, 0.08533602, 0.23190303,\n",
       "        0.37878122, 0.08056235, 0.23370938, 0.36842175, 0.0803627 ,\n",
       "        0.22796526, 0.36980724, 0.08853035, 0.25860214, 0.41589622,\n",
       "        0.08773303, 0.2518908 , 0.41552625, 0.08587689, 0.25288801,\n",
       "        0.41388555, 0.09738612, 0.27812681, 0.46737032, 0.1025394 ,\n",
       "        0.28548861, 0.44131436, 0.08835616, 0.25643759, 0.44038572,\n",
       "        0.06518703, 0.17991185, 0.29099488, 0.06445999, 0.18099785,\n",
       "        0.29403844, 0.06425934, 0.17438307, 0.28464599, 0.07307134,\n",
       "        0.19880915, 0.33287511, 0.06797533, 0.19790397, 0.32603488,\n",
       "        0.07131772, 0.21272879, 0.32474422, 0.08029623, 0.26670561,\n",
       "        0.37426233, 0.07882934, 0.23190861, 0.37069478, 0.08187156,\n",
       "        0.22074065, 0.36484261, 0.08928866, 0.25078931, 0.41652861,\n",
       "        0.08721995, 0.24453998, 0.38956547, 0.08479733, 0.23576756,\n",
       "        0.39208927, 0.09502692, 0.26566825, 0.43535743, 0.08997316,\n",
       "        0.25346704, 0.42095623, 0.08865609, 0.26608014, 0.41977344,\n",
       "        0.0625176 , 0.18370481, 0.28789725, 0.06156974, 0.17273793,\n",
       "        0.29003396, 0.06356955, 0.17856259, 0.28968549, 0.07190456,\n",
       "        0.20081062, 0.33175325, 0.07128124, 0.19756637, 0.33147659,\n",
       "        0.07126122, 0.19853864, 0.32303348, 0.07968941, 0.22911601,\n",
       "        0.37918582, 0.07903118, 0.22590199, 0.3632277 , 0.07642746,\n",
       "        0.22619944, 0.35697093, 0.08594217, 0.25335698, 0.4258925 ,\n",
       "        0.0878376 , 0.24774842, 0.39676595, 0.08511801, 0.23345013,\n",
       "        0.38345165, 0.09397388, 0.26460867, 0.4376574 , 0.09029784,\n",
       "        0.25892057, 0.42425694, 0.08825989, 0.25170426, 0.4073657 ,\n",
       "        0.06125612, 0.17274466, 0.29055886, 0.06128378, 0.17131543,\n",
       "        0.28248329, 0.06066575, 0.17197838, 0.29437361, 0.07537231,\n",
       "        0.21050253, 0.34771175, 0.07118311, 0.20400901, 0.33961487,\n",
       "        0.07090983, 0.19978952, 0.33482003, 0.08263712, 0.2290329 ,\n",
       "        0.37973013, 0.08401499, 0.22527795, 0.37078495, 0.07789683,\n",
       "        0.21508036, 0.35099635, 0.08835721, 0.24913983, 0.40658841,\n",
       "        0.08678408, 0.24569654, 0.43193097, 0.10492325, 0.24765191,\n",
       "        0.40599341, 0.0987349 , 0.27151904, 0.44890804, 0.09430957,\n",
       "        0.25839138, 0.43337822, 0.08801932, 0.26187487, 0.42370939,\n",
       "        0.06626816, 0.17389894, 0.28129277, 0.06314106, 0.18119645,\n",
       "        0.29000058, 0.06404943, 0.17277279, 0.29147034, 0.07087412,\n",
       "        0.19841161, 0.32458963, 0.07055559, 0.19602818, 0.32267704,\n",
       "        0.07070823, 0.1966033 , 0.32421985, 0.0796217 , 0.22943592,\n",
       "        0.37242479, 0.07999806, 0.22125344, 0.37279005, 0.07811546,\n",
       "        0.2253511 , 0.38286753, 0.09679785, 0.26137304, 0.41445689,\n",
       "        0.08585649, 0.24980545, 0.39925861, 0.08631368, 0.23932819,\n",
       "        0.40085979, 0.09508843, 0.26513491, 0.4452508 , 0.08979068,\n",
       "        0.25942478, 0.42876134, 0.08855128, 0.24939499, 0.4150178 ,\n",
       "        0.06159663, 0.1719789 , 0.29492245, 0.05837383, 0.17124748,\n",
       "        0.28379879, 0.06459293, 0.17849045, 0.28500881, 0.07141461,\n",
       "        0.20756578, 0.34029455, 0.07132297, 0.20093269, 0.33339849,\n",
       "        0.07219658, 0.19529305, 0.32139254, 0.08144951, 0.22892885,\n",
       "        0.37509604, 0.07995119, 0.22789726, 0.37940378, 0.08353024,\n",
       "        0.23231473, 0.37628713, 0.08852024, 0.25650492, 0.40904245,\n",
       "        0.08282032, 0.24494047, 0.40268936, 0.08358226, 0.23695369,\n",
       "        0.39292212, 0.09150462, 0.26326504, 0.44000545, 0.09080024,\n",
       "        0.26322865, 0.47129402, 0.10407367, 0.33637323, 0.45369935,\n",
       "        0.06171875, 0.16240559, 0.26571259, 0.05765467, 0.1619956 ,\n",
       "        0.26968036, 0.05916238, 0.1621376 , 0.26060104, 0.06385026,\n",
       "        0.17811694, 0.2967073 , 0.06630116, 0.18864107, 0.47462649,\n",
       "        0.07645421, 0.18340173, 0.32222123, 0.07240272, 0.32005358,\n",
       "        0.34550595, 0.07206678, 0.26147904, 0.41674533, 0.11745906,\n",
       "        0.30240583, 0.44052219, 0.10649681, 0.31767731, 0.50039787,\n",
       "        0.12370605, 0.29190235, 0.37063303, 0.08844256, 0.2053957 ,\n",
       "        0.34391284, 0.08355894, 0.22697573, 0.37616067, 0.08227139,\n",
       "        0.2230134 , 0.35547848, 0.07999334, 0.21730013, 0.35126901,\n",
       "        0.05604963, 0.15823021, 0.2751369 , 0.05945363, 0.15888276,\n",
       "        0.27613764, 0.06257882, 0.1657105 , 0.26141109, 0.06438069,\n",
       "        0.18444266, 0.30180268, 0.06342711, 0.17707205, 0.29746132,\n",
       "        0.06269836, 0.17847257, 0.28953147, 0.07171221, 0.20100608,\n",
       "        0.32873321, 0.07023149, 0.19401827, 0.32377982, 0.07011471,\n",
       "        0.19816322, 0.32383995, 0.07843776, 0.21218362, 0.35108767,\n",
       "        0.07752428, 0.20945764, 0.34048295, 0.0778358 , 0.20536118,\n",
       "        0.33267879, 0.08548732, 0.23057141, 0.37053623, 0.07907739,\n",
       "        0.22316294, 0.35821891, 0.0773138 , 0.21620893, 0.3591713 ,\n",
       "        0.0568851 , 0.16291242, 0.26750889, 0.05818772, 0.15993323,\n",
       "        0.26670256, 0.0591207 , 0.16535931, 0.25904703, 0.06779389,\n",
       "        0.18307161, 0.29840522, 0.06452198, 0.18514943, 0.30261073,\n",
       "        0.06372762, 0.18159022, 0.29835849, 0.07102242, 0.20008359,\n",
       "        0.33298092, 0.0705709 , 0.20209002, 0.32606535, 0.0701314 ,\n",
       "        0.19143896, 0.31312289, 0.07486362, 0.21426845, 0.35037284,\n",
       "        0.07825193, 0.2107842 , 0.34054179, 0.0760468 , 0.20243163,\n",
       "        0.34224386, 0.08082767, 0.2300796 , 0.37561116, 0.07913117,\n",
       "        0.21789651, 0.36546655, 0.07688298, 0.21337385, 0.35632949,\n",
       "        0.05761714, 0.16454849, 0.27233086, 0.05851645, 0.16705585,\n",
       "        0.2745995 , 0.05895276, 0.15894079, 0.25989265, 0.06679659,\n",
       "        0.18929844, 0.30274963, 0.06787415, 0.18425007, 0.29638543,\n",
       "        0.06206074, 0.18412318, 0.29460626, 0.07111888, 0.20452838,\n",
       "        0.33079948, 0.07057471, 0.19776559, 0.33608856, 0.07427979,\n",
       "        0.19635978, 0.31867948, 0.07589808, 0.21362891, 0.36197653,\n",
       "        0.07955565, 0.21577282, 0.3543468 , 0.07499127, 0.21148987,\n",
       "        0.33648491, 0.08035951, 0.23442135, 0.38013372, 0.08034687,\n",
       "        0.22060728, 0.37033815, 0.07947655, 0.21764946, 0.36421914,\n",
       "        0.06186872, 0.16517358, 0.26914163, 0.06288705, 0.16276355,\n",
       "        0.27337708, 0.0611074 , 0.18837404, 0.26170306, 0.06541781,\n",
       "        0.19163084, 0.30724258, 0.06737604, 0.18117261, 0.30215378,\n",
       "        0.07280593, 0.20436897, 0.30296597, 0.073206  , 0.19744205,\n",
       "        0.32948923, 0.07006097, 0.19758697, 0.32906823, 0.07176142,\n",
       "        0.19555922, 0.32110591, 0.07492671, 0.21515374, 0.35699983,\n",
       "        0.07375979, 0.21580868, 0.34731469, 0.07422667, 0.21303663,\n",
       "        0.34514823, 0.07971745, 0.2279284 , 0.38428893, 0.08101773,\n",
       "        0.2222836 , 0.37405076, 0.07673473, 0.21927047, 0.33686576]),\n",
       " 'std_fit_time': array([0.00090364, 0.00215239, 0.00585729, 0.00227892, 0.00421717,\n",
       "        0.00345785, 0.00168889, 0.00245073, 0.01037039, 0.00227511,\n",
       "        0.01140532, 0.0070434 , 0.00298228, 0.00421163, 0.02996374,\n",
       "        0.00708298, 0.01165458, 0.00382406, 0.00411322, 0.00411938,\n",
       "        0.01055069, 0.00288225, 0.0032886 , 0.00439972, 0.00255664,\n",
       "        0.00316733, 0.00795345, 0.00249541, 0.00660648, 0.00463719,\n",
       "        0.00525398, 0.00629181, 0.0091691 , 0.00199231, 0.00422341,\n",
       "        0.00595072, 0.00361968, 0.00644817, 0.01599601, 0.0069709 ,\n",
       "        0.00907943, 0.00667186, 0.00315927, 0.00135648, 0.02136295,\n",
       "        0.00317349, 0.005981  , 0.00567892, 0.00391211, 0.00443468,\n",
       "        0.00386914, 0.0019432 , 0.00333376, 0.0091326 , 0.0052984 ,\n",
       "        0.00310402, 0.00688571, 0.00338897, 0.00222991, 0.00548456,\n",
       "        0.00278253, 0.01031135, 0.00849388, 0.00185872, 0.00168149,\n",
       "        0.00428628, 0.00072482, 0.00488791, 0.00393699, 0.0026811 ,\n",
       "        0.00239494, 0.00492387, 0.00934215, 0.00673811, 0.00548469,\n",
       "        0.00308721, 0.00539916, 0.00552261, 0.00214305, 0.00343783,\n",
       "        0.00446453, 0.00279779, 0.00288806, 0.00730528, 0.00151565,\n",
       "        0.00742885, 0.01420398, 0.00682308, 0.00155251, 0.01322544,\n",
       "        0.00308513, 0.00236845, 0.0061117 , 0.00275063, 0.00212211,\n",
       "        0.00621877, 0.00088054, 0.00300121, 0.00843063, 0.00186083,\n",
       "        0.00890391, 0.00808901, 0.0036637 , 0.00392496, 0.00477054,\n",
       "        0.00288139, 0.00304305, 0.00427491, 0.00306983, 0.00345597,\n",
       "        0.00625082, 0.00252108, 0.00242225, 0.00618802, 0.00257945,\n",
       "        0.00711825, 0.00460004, 0.00237087, 0.00360224, 0.00579606,\n",
       "        0.00351557, 0.00368977, 0.00589076, 0.00562613, 0.00339877,\n",
       "        0.00492725, 0.00412751, 0.00484708, 0.00728184, 0.00232103,\n",
       "        0.01045996, 0.00542966, 0.00309219, 0.00587032, 0.00821858,\n",
       "        0.00353014, 0.00174644, 0.00764293, 0.00373177, 0.00386813,\n",
       "        0.00515091, 0.00176562, 0.00507899, 0.00257458, 0.00217509,\n",
       "        0.00403924, 0.00553059, 0.00263795, 0.01058713, 0.00315995,\n",
       "        0.00235205, 0.00250239, 0.00428078, 0.00285496, 0.00451609,\n",
       "        0.05559101, 0.02222889, 0.00297122, 0.01355345, 0.00572975,\n",
       "        0.01646275, 0.00637759, 0.00185499, 0.00291422, 0.0117859 ,\n",
       "        0.00258921, 0.0028605 , 0.01350035, 0.00777747, 0.00317926,\n",
       "        0.00486659, 0.00203177, 0.00451542, 0.01185346, 0.00281287,\n",
       "        0.00342467, 0.0073851 , 0.00078428, 0.00388367, 0.00625513,\n",
       "        0.00243727, 0.00355798, 0.01400014, 0.00343905, 0.00498553,\n",
       "        0.00519129, 0.00332315, 0.00475982, 0.00655857, 0.00326395,\n",
       "        0.00961608, 0.003578  , 0.00242906, 0.00378052, 0.01008205,\n",
       "        0.00407248, 0.00224787, 0.00678503, 0.00194849, 0.00622402,\n",
       "        0.00553217, 0.00465903, 0.00286258, 0.00341377, 0.0038854 ,\n",
       "        0.00475888, 0.00934038, 0.00911302, 0.00540065, 0.00632014,\n",
       "        0.00369204, 0.00930979, 0.00611455, 0.00261047, 0.00352277,\n",
       "        0.00360524, 0.00531408, 0.00675805, 0.02560921, 0.02055144,\n",
       "        0.01417119, 0.00593568, 0.00134488, 0.00310209, 0.01988116,\n",
       "        0.01135829, 0.00354139, 0.00520809, 0.00282672, 0.00434206,\n",
       "        0.0117477 , 0.00212775, 0.00384855, 0.0109549 , 0.00323463,\n",
       "        0.00516572, 0.00692892, 0.00147921, 0.00383694, 0.00820091,\n",
       "        0.00586285, 0.00808986, 0.0073912 , 0.0038471 , 0.02179493,\n",
       "        0.00689215, 0.00384147, 0.01033303, 0.00530603, 0.00381162,\n",
       "        0.00445286, 0.00715618, 0.00153799, 0.00331974, 0.01139149,\n",
       "        0.00359992, 0.00847165, 0.00691026, 0.00343467, 0.00223513,\n",
       "        0.0062105 , 0.00474544, 0.0049067 , 0.0064297 , 0.00119604,\n",
       "        0.00417792, 0.0090523 , 0.00266212, 0.01358091, 0.01401262,\n",
       "        0.00253919, 0.00392632, 0.00223146, 0.00140563, 0.00238163,\n",
       "        0.00621833, 0.00265864, 0.00309969, 0.00464173, 0.00215978,\n",
       "        0.00242606, 0.00486908, 0.00217815, 0.00336444, 0.00920387,\n",
       "        0.00109389, 0.00396993, 0.00445729, 0.00139726, 0.00597388,\n",
       "        0.00738308, 0.00303966, 0.00405545, 0.00638535, 0.00068577,\n",
       "        0.00414023, 0.00630065, 0.00296422, 0.00756583, 0.00734634,\n",
       "        0.00367675, 0.00950748, 0.00205002, 0.00306926, 0.00287891,\n",
       "        0.00378815, 0.00307635, 0.00640098, 0.0036773 , 0.001082  ,\n",
       "        0.00491968, 0.00558297, 0.00241809, 0.00419562, 0.0045298 ,\n",
       "        0.00125852, 0.00347231, 0.00326631, 0.0019756 , 0.00310903,\n",
       "        0.0029642 , 0.00127164, 0.00314161, 0.00626733, 0.00367581,\n",
       "        0.00477661, 0.00376765, 0.00087821, 0.00441603, 0.00432408,\n",
       "        0.00210091, 0.00458928, 0.00494068, 0.00611222, 0.00401752,\n",
       "        0.00902231, 0.00372427, 0.00452886, 0.00499179, 0.0032977 ,\n",
       "        0.00428887, 0.0088307 , 0.00292673, 0.00620601, 0.00671813,\n",
       "        0.00389537, 0.00513621, 0.02717936, 0.00354785, 0.00497599,\n",
       "        0.00629212, 0.00307478, 0.0040756 , 0.00739   , 0.00312622,\n",
       "        0.00890605, 0.0151976 , 0.00332869, 0.01004335, 0.00416659,\n",
       "        0.00438886, 0.00253387, 0.00581487, 0.00202617, 0.00607724,\n",
       "        0.00336674, 0.001957  , 0.00377848, 0.00325318, 0.0027097 ,\n",
       "        0.00386045, 0.00368292, 0.00226839, 0.00209241, 0.00274004,\n",
       "        0.0048949 , 0.00176095, 0.00361927, 0.0027193 , 0.01009033,\n",
       "        0.003099  , 0.00297054, 0.00384789, 0.00445414, 0.00081093,\n",
       "        0.0034123 , 0.005212  , 0.00219531, 0.00705285, 0.00399002,\n",
       "        0.0024517 , 0.0033669 , 0.00550165, 0.00322865, 0.00351421,\n",
       "        0.00762564, 0.00228827, 0.00220675, 0.00649133, 0.00077781,\n",
       "        0.00369681, 0.00665832, 0.00244117, 0.00191362, 0.00815314,\n",
       "        0.00219246, 0.00350619, 0.01848808, 0.00321793, 0.00653918,\n",
       "        0.00681435, 0.00451321, 0.00863135, 0.00524902, 0.00105969,\n",
       "        0.01056043, 0.00691814, 0.00173946, 0.00405326, 0.00724574,\n",
       "        0.00640786, 0.00205447, 0.00229301, 0.00326929, 0.00264314,\n",
       "        0.00909307, 0.00314064, 0.00836229, 0.01010718, 0.00311386,\n",
       "        0.00168563, 0.00267934, 0.0007895 , 0.0039822 , 0.00633337,\n",
       "        0.00095567, 0.00363941, 0.00576706, 0.00244005, 0.0019891 ,\n",
       "        0.00624664, 0.00221626, 0.00350721, 0.00751073, 0.001952  ,\n",
       "        0.00531793, 0.06541558, 0.00779495, 0.05580559, 0.02804518,\n",
       "        0.00445695, 0.01093927, 0.00444544, 0.0018277 , 0.00237098,\n",
       "        0.00936504, 0.00136496, 0.0027573 , 0.00208749, 0.00193177,\n",
       "        0.00156465, 0.00380147, 0.00113281, 0.00501074, 0.07563201,\n",
       "        0.0173722 , 0.00555871, 0.00846339, 0.00601385, 0.05248281,\n",
       "        0.01955793, 0.00368117, 0.03615692, 0.07758444, 0.0098792 ,\n",
       "        0.01725575, 0.04241072, 0.02715126, 0.05546137, 0.04799947,\n",
       "        0.02965099, 0.05907862, 0.02330847, 0.00522584, 0.00694541,\n",
       "        0.00612361, 0.00431068, 0.00458703, 0.00857247, 0.00450974,\n",
       "        0.00280947, 0.00197096, 0.00447707, 0.00514615, 0.00429594,\n",
       "        0.00107588, 0.00180086, 0.01519081, 0.0030281 , 0.00573892,\n",
       "        0.00461548, 0.00416245, 0.00369268, 0.00789168, 0.00195685,\n",
       "        0.00390145, 0.00854763, 0.0018132 , 0.00404961, 0.009285  ,\n",
       "        0.00265274, 0.00481619, 0.01054623, 0.00180729, 0.00333236,\n",
       "        0.00413288, 0.00206872, 0.0037853 , 0.00769724, 0.00227899,\n",
       "        0.00269573, 0.00439383, 0.00315487, 0.00300223, 0.00757856,\n",
       "        0.00223568, 0.00183772, 0.00685676, 0.00396679, 0.00345864,\n",
       "        0.00272223, 0.00381411, 0.00387532, 0.00637137, 0.00136106,\n",
       "        0.00895691, 0.00918132, 0.00421525, 0.00228248, 0.00890991,\n",
       "        0.00122037, 0.00354609, 0.00645415, 0.00206678, 0.00150198,\n",
       "        0.01344551, 0.00346002, 0.00514882, 0.00446471, 0.0030781 ,\n",
       "        0.0104889 , 0.0101409 , 0.00157407, 0.00929797, 0.00860249,\n",
       "        0.0019864 , 0.0044035 , 0.00523213, 0.00205903, 0.00302115,\n",
       "        0.00237417, 0.00270739, 0.00605614, 0.0031733 , 0.00318177,\n",
       "        0.0036072 , 0.00279438, 0.00111594, 0.00294425, 0.00778402,\n",
       "        0.00386987, 0.00317511, 0.00356542, 0.00396285, 0.0022531 ,\n",
       "        0.00685686, 0.00208511, 0.00282004, 0.00523988, 0.00210163,\n",
       "        0.00293112, 0.0054952 , 0.00330251, 0.00390887, 0.00558303,\n",
       "        0.00103507, 0.00439579, 0.00426575, 0.00485666, 0.00681548,\n",
       "        0.00471728, 0.0021832 , 0.00184101, 0.00365514, 0.00303328,\n",
       "        0.00413309, 0.006722  , 0.00344331, 0.00300524, 0.00572835,\n",
       "        0.00033016, 0.00485764, 0.00496412, 0.00139367, 0.00313938,\n",
       "        0.00518658, 0.00283638, 0.00240301, 0.00542259, 0.00442167,\n",
       "        0.0020571 , 0.00289957, 0.00247745, 0.00620418, 0.00435885,\n",
       "        0.00407028, 0.00435689, 0.00813311, 0.00378632, 0.00478231,\n",
       "        0.0040832 , 0.00195329, 0.00611688, 0.00717403, 0.00193747,\n",
       "        0.00357947, 0.00858733, 0.00234763, 0.00189379, 0.009034  ,\n",
       "        0.00329049, 0.00339634, 0.00507035, 0.00472512, 0.00339159,\n",
       "        0.02541548, 0.00660436, 0.01509325, 0.00509103, 0.00304064,\n",
       "        0.02256987, 0.00900264, 0.0013557 , 0.00089939, 0.00758416,\n",
       "        0.0083033 , 0.02489314, 0.00166326, 0.00289514, 0.00404147,\n",
       "        0.00486357, 0.00334997, 0.0025768 , 0.00787505, 0.00528781,\n",
       "        0.00683614, 0.00478176, 0.00152928, 0.00157197, 0.00509656,\n",
       "        0.00289035, 0.00170112, 0.00590492, 0.0044157 , 0.00411169,\n",
       "        0.00797312, 0.002338  , 0.00584966, 0.0088904 , 0.00221189,\n",
       "        0.00325121, 0.00304977, 0.00279163, 0.00532436, 0.04824688]),\n",
       " 'mean_score_time': array([0.00503502, 0.00550985, 0.0065506 , 0.00385599, 0.00478263,\n",
       "        0.0061944 , 0.00400882, 0.0047142 , 0.00632467, 0.00384784,\n",
       "        0.00554276, 0.00736594, 0.00403843, 0.00522728, 0.00873876,\n",
       "        0.00403185, 0.00562634, 0.00699806, 0.00404553, 0.00567513,\n",
       "        0.00796561, 0.00418987, 0.00583763, 0.00753579, 0.00402684,\n",
       "        0.00582294, 0.00746484, 0.00417848, 0.00663733, 0.00836987,\n",
       "        0.00435309, 0.00650773, 0.00964503, 0.00438414, 0.00616703,\n",
       "        0.00787945, 0.00446181, 0.00690236, 0.00894051, 0.00626764,\n",
       "        0.00628462, 0.00853434, 0.004074  , 0.00632825, 0.00917764,\n",
       "        0.0035996 , 0.00483761, 0.00653682, 0.0041801 , 0.00506148,\n",
       "        0.00641499, 0.00365562, 0.00475831, 0.00684943, 0.0046391 ,\n",
       "        0.00567026, 0.00751014, 0.00398593, 0.00519419, 0.00784974,\n",
       "        0.00394478, 0.00616016, 0.00703793, 0.00430384, 0.00640464,\n",
       "        0.00816927, 0.00425181, 0.00598674, 0.00777082, 0.00390391,\n",
       "        0.00549884, 0.00759158, 0.0040628 , 0.00640535, 0.00889797,\n",
       "        0.00415235, 0.00632524, 0.00809398, 0.00426564, 0.00609245,\n",
       "        0.00868654, 0.00409265, 0.0069159 , 0.00918593, 0.00497122,\n",
       "        0.00647397, 0.00950818, 0.00396037, 0.0060925 , 0.00818086,\n",
       "        0.003685  , 0.00481977, 0.00602136, 0.00376344, 0.00492587,\n",
       "        0.00617232, 0.00383468, 0.0049593 , 0.00682077, 0.00387588,\n",
       "        0.00574574, 0.00828805, 0.00416608, 0.00586247, 0.00724435,\n",
       "        0.00429711, 0.00542717, 0.00658884, 0.00409622, 0.00607395,\n",
       "        0.00943284, 0.00551605, 0.00631609, 0.00833378, 0.00395708,\n",
       "        0.00567341, 0.00784912, 0.00422511, 0.0068872 , 0.00886712,\n",
       "        0.00426645, 0.00643582, 0.00880485, 0.00449638, 0.00612206,\n",
       "        0.00819402, 0.00440178, 0.00718522, 0.00977769, 0.00417233,\n",
       "        0.0065742 , 0.00856638, 0.00421176, 0.00648623, 0.00835505,\n",
       "        0.00384378, 0.00484271, 0.00631404, 0.003969  , 0.00483179,\n",
       "        0.00617204, 0.00367131, 0.00476503, 0.00638084, 0.00385633,\n",
       "        0.00546451, 0.00748959, 0.00398684, 0.00676246, 0.00720806,\n",
       "        0.00376844, 0.00561748, 0.00717916, 0.00416703, 0.00591035,\n",
       "        0.01119466, 0.00408711, 0.00581999, 0.00827589, 0.00425396,\n",
       "        0.00594416, 0.00772047, 0.00432553, 0.00643725, 0.00935163,\n",
       "        0.00406027, 0.00638576, 0.00963583, 0.00494637, 0.00640683,\n",
       "        0.00798464, 0.00424581, 0.00673828, 0.00939665, 0.00427761,\n",
       "        0.00651741, 0.00888524, 0.00444012, 0.00655484, 0.00843801,\n",
       "        0.00453863, 0.00464482, 0.00568066, 0.00636864, 0.00493865,\n",
       "        0.00604391, 0.00461211, 0.00493588, 0.00683217, 0.00407562,\n",
       "        0.00539899, 0.00755038, 0.00392284, 0.00562778, 0.00807986,\n",
       "        0.00388083, 0.00527215, 0.00707998, 0.00423584, 0.00647135,\n",
       "        0.00834317, 0.00399399, 0.00650477, 0.0077878 , 0.0038734 ,\n",
       "        0.00599022, 0.00773168, 0.00408163, 0.00631557, 0.00875068,\n",
       "        0.00602846, 0.0064043 , 0.0088985 , 0.00412793, 0.0064992 ,\n",
       "        0.00798798, 0.00412731, 0.00698791, 0.01260934, 0.00424781,\n",
       "        0.00637136, 0.00881348, 0.00421143, 0.0061738 , 0.00895553,\n",
       "        0.00398903, 0.00471458, 0.00665674, 0.00395741, 0.00486412,\n",
       "        0.00679913, 0.00385685, 0.00473709, 0.0063972 , 0.00402803,\n",
       "        0.00523849, 0.00747271, 0.00380988, 0.00538716, 0.00754175,\n",
       "        0.00372968, 0.00549111, 0.00763154, 0.00421138, 0.00713086,\n",
       "        0.00778532, 0.00433903, 0.00586171, 0.00803771, 0.00536456,\n",
       "        0.0057548 , 0.00832276, 0.00425749, 0.00630183, 0.00856543,\n",
       "        0.0042376 , 0.00708494, 0.00814762, 0.00403881, 0.00583925,\n",
       "        0.00790362, 0.00472283, 0.00666122, 0.00891743, 0.00456777,\n",
       "        0.00694199, 0.00844836, 0.0042428 , 0.00622602, 0.00871496,\n",
       "        0.00369458, 0.00482759, 0.00590544, 0.00368104, 0.0047935 ,\n",
       "        0.00687404, 0.00392547, 0.00498457, 0.00633998, 0.00391073,\n",
       "        0.00583777, 0.00739679, 0.00400167, 0.00532765, 0.00743871,\n",
       "        0.00401583, 0.00532966, 0.00703044, 0.00407557, 0.00608501,\n",
       "        0.00812812, 0.00437083, 0.00600438, 0.0076571 , 0.00396876,\n",
       "        0.00577054, 0.00753183, 0.00426364, 0.00695782, 0.00953107,\n",
       "        0.00419135, 0.00641441, 0.00801539, 0.00420079, 0.00600929,\n",
       "        0.00825167, 0.00425472, 0.00698118, 0.00907788, 0.00398951,\n",
       "        0.00675383, 0.00872664, 0.00407338, 0.00637736, 0.00806322,\n",
       "        0.00412116, 0.00482273, 0.00628357, 0.00373516, 0.00472574,\n",
       "        0.00602932, 0.00373063, 0.00486288, 0.00642023, 0.00410061,\n",
       "        0.00560541, 0.00763512, 0.00393691, 0.00572324, 0.00727754,\n",
       "        0.00395837, 0.00534286, 0.00715437, 0.00410967, 0.00586185,\n",
       "        0.00977201, 0.00416503, 0.00616288, 0.0078938 , 0.00390358,\n",
       "        0.00553708, 0.0076654 , 0.00417471, 0.006774  , 0.00855045,\n",
       "        0.00436926, 0.00665879, 0.01091542, 0.00529737, 0.0061738 ,\n",
       "        0.00847921, 0.00472503, 0.00695558, 0.00987577, 0.00534172,\n",
       "        0.00681982, 0.0085722 , 0.00406361, 0.00770226, 0.00940404,\n",
       "        0.00399528, 0.00492349, 0.006323  , 0.00415826, 0.0052629 ,\n",
       "        0.00723023, 0.00392447, 0.00493479, 0.00618815, 0.00385094,\n",
       "        0.00567799, 0.00853333, 0.00429454, 0.00540318, 0.00729642,\n",
       "        0.00374379, 0.00540018, 0.00822721, 0.00408568, 0.00651693,\n",
       "        0.00767069, 0.00390735, 0.00593371, 0.00790215, 0.00402112,\n",
       "        0.0058866 , 0.00853829, 0.00435238, 0.00682788, 0.00881162,\n",
       "        0.00429635, 0.00646882, 0.00985603, 0.00407228, 0.00600009,\n",
       "        0.00790482, 0.00416579, 0.0067286 , 0.01096706, 0.00424366,\n",
       "        0.00710683, 0.00875711, 0.00418696, 0.00624156, 0.00801096,\n",
       "        0.0036675 , 0.00482221, 0.00637546, 0.00338631, 0.0048151 ,\n",
       "        0.00597477, 0.00414457, 0.00522032, 0.00658278, 0.00443711,\n",
       "        0.00586462, 0.00820446, 0.00431967, 0.00523291, 0.00734587,\n",
       "        0.0036325 , 0.00551391, 0.00749979, 0.00414014, 0.00631456,\n",
       "        0.00787759, 0.00526996, 0.00583668, 0.00822382, 0.00413437,\n",
       "        0.00591111, 0.00791626, 0.0043406 , 0.00624356, 0.0080935 ,\n",
       "        0.00417943, 0.00617061, 0.00814152, 0.00394669, 0.00599532,\n",
       "        0.00822115, 0.00397854, 0.00644374, 0.00942554, 0.00417218,\n",
       "        0.0065578 , 0.01003942, 0.00584397, 0.00719252, 0.00920386,\n",
       "        0.00482168, 0.00487313, 0.00623479, 0.00370641, 0.0047039 ,\n",
       "        0.00638509, 0.00374012, 0.00516834, 0.00620556, 0.00448694,\n",
       "        0.00514784, 0.00734663, 0.00376577, 0.00531979, 0.01127295,\n",
       "        0.00410042, 0.00615621, 0.00738134, 0.00532064, 0.00894356,\n",
       "        0.00884542, 0.00421004, 0.00679078, 0.01313019, 0.00942216,\n",
       "        0.00824771, 0.011765  , 0.00791078, 0.00791545, 0.00884995,\n",
       "        0.01277862, 0.00621057, 0.0089242 , 0.00546565, 0.00573401,\n",
       "        0.00802059, 0.00424194, 0.00637703, 0.00828319, 0.0046566 ,\n",
       "        0.00606265, 0.00805116, 0.00409107, 0.00644264, 0.00752277,\n",
       "        0.0038137 , 0.00465412, 0.00748763, 0.00526471, 0.0049612 ,\n",
       "        0.00635338, 0.00417519, 0.00480042, 0.00617623, 0.00389714,\n",
       "        0.00547695, 0.0060029 , 0.00582385, 0.00545278, 0.00702901,\n",
       "        0.0038518 , 0.00506806, 0.00682015, 0.00383735, 0.00612798,\n",
       "        0.00780683, 0.00392265, 0.0054769 , 0.00762539, 0.00438061,\n",
       "        0.0056314 , 0.00746884, 0.00393982, 0.00586662, 0.00875907,\n",
       "        0.00423422, 0.00567117, 0.00794125, 0.00433474, 0.00567298,\n",
       "        0.00775261, 0.0042665 , 0.00642037, 0.00855498, 0.00411849,\n",
       "        0.0062283 , 0.00847688, 0.00410628, 0.00630965, 0.0083849 ,\n",
       "        0.00361214, 0.00595622, 0.00594573, 0.0046454 , 0.00472598,\n",
       "        0.00627813, 0.00393729, 0.0049015 , 0.00615172, 0.00384111,\n",
       "        0.00549979, 0.00806475, 0.00393238, 0.00533104, 0.00703006,\n",
       "        0.00445347, 0.00538197, 0.00682354, 0.00381279, 0.00585322,\n",
       "        0.00845308, 0.00400825, 0.00585098, 0.00758176, 0.00401163,\n",
       "        0.0054379 , 0.00835218, 0.00403318, 0.00703936, 0.0095108 ,\n",
       "        0.00448756, 0.00575595, 0.00790744, 0.00400381, 0.00608821,\n",
       "        0.00797267, 0.00425205, 0.00704026, 0.00845809, 0.00472417,\n",
       "        0.00607905, 0.00807495, 0.00410838, 0.00587182, 0.00796819,\n",
       "        0.00373559, 0.0050035 , 0.00596356, 0.00388532, 0.00509   ,\n",
       "        0.00672064, 0.00391417, 0.00483723, 0.00629058, 0.0040544 ,\n",
       "        0.0055409 , 0.00738063, 0.00487633, 0.00636659, 0.00719528,\n",
       "        0.00392208, 0.00582905, 0.00716977, 0.00423894, 0.00563216,\n",
       "        0.00768819, 0.00410824, 0.00584559, 0.00790582, 0.00395689,\n",
       "        0.00571179, 0.00829411, 0.00425811, 0.00616503, 0.00853729,\n",
       "        0.00420766, 0.00726829, 0.00819845, 0.00511656, 0.00579672,\n",
       "        0.00786176, 0.00422258, 0.00638041, 0.00879426, 0.00417337,\n",
       "        0.00604615, 0.00908709, 0.00419655, 0.00613813, 0.00816126,\n",
       "        0.00366745, 0.00525389, 0.00647111, 0.00407424, 0.00468845,\n",
       "        0.00609756, 0.00370173, 0.00514507, 0.00593238, 0.00454698,\n",
       "        0.00552678, 0.0079452 , 0.00516815, 0.00526781, 0.00720954,\n",
       "        0.00465136, 0.00665431, 0.00694127, 0.00396423, 0.00576253,\n",
       "        0.00840521, 0.00416508, 0.00561237, 0.00750742, 0.00386314,\n",
       "        0.00556378, 0.00735965, 0.00408025, 0.00582557, 0.00838461,\n",
       "        0.00407577, 0.00585847, 0.00806298, 0.00496325, 0.00588861,\n",
       "        0.00866628, 0.0039712 , 0.0061224 , 0.00961218, 0.00417147,\n",
       "        0.00618439, 0.00822425, 0.0039537 , 0.00593901, 0.00729327]),\n",
       " 'std_score_time': array([1.43704492e-03, 1.57055645e-03, 4.72112577e-04, 7.58502996e-05,\n",
       "        1.09752066e-04, 3.33982185e-04, 6.41437233e-04, 1.25967472e-04,\n",
       "        4.45991165e-04, 1.30196629e-04, 1.90796187e-04, 1.13776084e-04,\n",
       "        1.61288969e-04, 1.23929363e-04, 3.28317274e-03, 1.81563874e-04,\n",
       "        6.84865402e-04, 4.26064874e-04, 1.53538485e-04, 1.14064830e-04,\n",
       "        4.42567294e-04, 2.68997640e-04, 1.30236563e-04, 1.88230319e-04,\n",
       "        2.05218736e-04, 3.64721936e-04, 2.97908379e-05, 3.31816736e-04,\n",
       "        6.42050335e-04, 1.37390630e-04, 4.57796314e-04, 6.57748156e-04,\n",
       "        2.20653559e-03, 4.40882450e-04, 3.30001342e-04, 2.09069871e-04,\n",
       "        5.58404010e-04, 3.44256733e-04, 3.77776579e-04, 3.40703915e-03,\n",
       "        8.13470216e-05, 1.93400272e-04, 1.22896450e-04, 3.21271228e-04,\n",
       "        2.76324600e-03, 6.79417489e-05, 2.30116573e-04, 8.20278220e-04,\n",
       "        6.21495778e-04, 2.82966701e-04, 5.25814057e-04, 8.83431919e-05,\n",
       "        3.14254152e-04, 4.71096835e-04, 9.58035931e-04, 3.00704544e-04,\n",
       "        4.17522014e-04, 3.45962772e-04, 1.21073265e-04, 1.54813410e-03,\n",
       "        8.94820978e-05, 5.30643890e-04, 1.63613968e-04, 3.57698008e-04,\n",
       "        4.02922263e-04, 6.45293221e-04, 2.16985041e-04, 2.42017496e-04,\n",
       "        8.58314037e-05, 5.04040560e-05, 1.32105099e-04, 2.36707297e-04,\n",
       "        6.90402903e-05, 1.87397428e-04, 7.96917450e-04, 1.48486004e-04,\n",
       "        3.32841661e-04, 2.88325832e-04, 5.06333048e-04, 1.11704496e-04,\n",
       "        1.75896808e-03, 8.14650014e-05, 7.06309809e-04, 5.54998727e-04,\n",
       "        8.55747518e-04, 5.26657486e-04, 1.19317717e-03, 3.78986682e-04,\n",
       "        3.58595315e-04, 2.46890745e-04, 1.45580969e-04, 5.26850957e-04,\n",
       "        2.41078587e-04, 2.62425977e-04, 1.99742447e-04, 3.54790514e-04,\n",
       "        1.60962130e-04, 4.37042610e-04, 9.43953980e-04, 1.10683611e-04,\n",
       "        4.71946662e-04, 1.73268107e-03, 3.63039631e-04, 5.96223500e-04,\n",
       "        9.23244663e-05, 5.11549330e-04, 2.64671144e-04, 7.03535449e-04,\n",
       "        2.78850986e-04, 3.37015503e-04, 2.04963667e-03, 1.80082555e-03,\n",
       "        1.31976904e-03, 1.19271913e-03, 9.42852369e-05, 1.40687408e-04,\n",
       "        8.89817676e-04, 2.12760529e-04, 3.70544845e-04, 6.14552598e-04,\n",
       "        2.64415788e-04, 4.61259137e-04, 1.01821404e-03, 3.55307471e-04,\n",
       "        4.11293690e-04, 3.79389767e-04, 3.01507668e-04, 7.82853872e-04,\n",
       "        9.77047512e-04, 9.09178546e-05, 2.08947322e-04, 2.44040348e-04,\n",
       "        2.39247911e-04, 7.15667914e-04, 4.64075321e-04, 2.56010979e-04,\n",
       "        2.91043426e-04, 2.49093315e-04, 1.98565631e-04, 2.10672517e-04,\n",
       "        7.91460055e-05, 4.86501509e-05, 2.00899197e-04, 3.88464176e-04,\n",
       "        1.69132103e-04, 1.65904688e-04, 3.19965536e-04, 2.06668123e-04,\n",
       "        2.09432941e-03, 1.28501324e-04, 3.95736003e-04, 4.19791321e-04,\n",
       "        3.28945504e-04, 1.55010981e-04, 7.37590792e-05, 5.06830512e-03,\n",
       "        1.26660785e-04, 1.14441752e-04, 4.50759328e-04, 2.62793891e-04,\n",
       "        3.61743314e-04, 2.52163148e-04, 1.94905495e-04, 1.04306142e-04,\n",
       "        8.04661689e-04, 5.31011220e-05, 4.13499904e-04, 5.89058243e-04,\n",
       "        1.35994776e-03, 5.75277667e-04, 1.40268278e-04, 5.93568228e-05,\n",
       "        1.11752727e-04, 5.39464163e-04, 1.32666484e-04, 7.59309837e-05,\n",
       "        3.24577124e-04, 4.05627840e-04, 8.59943789e-05, 1.59581775e-04,\n",
       "        1.42976809e-03, 1.22361878e-04, 8.60499186e-04, 2.93926321e-03,\n",
       "        1.80755715e-04, 1.48739796e-04, 1.20577420e-03, 1.66389974e-04,\n",
       "        2.00529519e-03, 1.60401358e-04, 1.58000480e-04, 5.65924727e-04,\n",
       "        2.48475931e-04, 2.99196816e-04, 5.72177004e-04, 2.67480553e-04,\n",
       "        1.86417534e-04, 2.38569908e-04, 2.75532775e-04, 1.23616528e-03,\n",
       "        7.06242828e-04, 4.89904641e-05, 6.37450495e-04, 1.93926430e-04,\n",
       "        1.04197637e-04, 4.66166976e-04, 3.33911089e-04, 1.61708030e-04,\n",
       "        2.63583858e-04, 5.69915105e-04, 4.05386104e-03, 1.12380372e-04,\n",
       "        1.01325992e-03, 6.27096884e-05, 5.41702093e-04, 1.56995693e-04,\n",
       "        1.94640547e-04, 2.95484048e-04, 5.01942826e-03, 2.03023106e-04,\n",
       "        1.82489487e-04, 6.94510661e-05, 3.41612876e-04, 6.33388740e-05,\n",
       "        1.36562368e-03, 7.91895722e-04, 6.88980731e-04, 9.64000617e-04,\n",
       "        2.91538628e-04, 2.51589045e-04, 6.30747237e-04, 2.13333044e-04,\n",
       "        8.40895805e-05, 3.88760408e-04, 2.10415514e-04, 2.57686497e-04,\n",
       "        3.62426148e-04, 1.18648680e-04, 2.63757318e-04, 8.79838555e-04,\n",
       "        4.35064681e-04, 3.63342299e-04, 2.01979721e-03, 1.45914305e-04,\n",
       "        1.04419004e-03, 2.11684229e-04, 6.26473086e-04, 9.22215145e-05,\n",
       "        4.21537684e-04, 1.77305839e-03, 9.55380582e-05, 1.19016535e-03,\n",
       "        1.97303174e-04, 2.02688742e-04, 1.25275295e-04, 2.48848307e-04,\n",
       "        1.78518516e-03, 3.32361232e-04, 1.40391743e-04, 1.36509626e-04,\n",
       "        6.21093411e-04, 4.03762381e-04, 2.27769962e-04, 2.64737379e-04,\n",
       "        7.39731864e-04, 1.04972786e-03, 1.59288664e-04, 3.25013991e-04,\n",
       "        1.54777361e-04, 1.30703437e-03, 2.07605250e-04, 2.10433840e-04,\n",
       "        4.99347902e-04, 5.59569167e-05, 2.72009273e-04, 5.70490563e-04,\n",
       "        2.25902911e-04, 2.33460924e-04, 1.04712767e-04, 1.46882053e-04,\n",
       "        9.22100844e-04, 3.84817413e-04, 1.58329641e-04, 1.14436587e-04,\n",
       "        4.70794040e-04, 1.18336400e-04, 1.75933229e-04, 2.41551435e-04,\n",
       "        2.12885358e-04, 2.05924577e-04, 4.19942919e-04, 5.20468918e-04,\n",
       "        4.26167042e-04, 1.86056665e-04, 8.97800165e-05, 2.53297433e-04,\n",
       "        1.97845156e-04, 3.05444331e-04, 5.16944976e-04, 1.00053012e-03,\n",
       "        1.59839727e-04, 1.24004068e-04, 2.18412656e-04, 2.76007525e-04,\n",
       "        1.78389197e-04, 8.73929642e-04, 1.71550450e-04, 4.67342243e-04,\n",
       "        2.95754287e-04, 6.42051989e-04, 3.21190608e-04, 4.13971978e-04,\n",
       "        1.25217202e-04, 4.29459690e-04, 2.34248922e-04, 9.43497414e-04,\n",
       "        1.93164432e-04, 2.91667745e-04, 3.41841363e-05, 1.27274028e-04,\n",
       "        1.71214582e-04, 2.14457935e-04, 2.22375942e-04, 1.31589830e-04,\n",
       "        1.13258653e-04, 1.41930594e-04, 7.45554636e-04, 1.04865544e-04,\n",
       "        3.34212235e-04, 2.80204194e-04, 4.94348376e-05, 9.16657396e-05,\n",
       "        2.47531428e-04, 2.12214522e-04, 1.77400252e-04, 2.61342804e-03,\n",
       "        1.18269900e-04, 4.23258427e-04, 4.29551443e-04, 1.07272507e-04,\n",
       "        9.71196143e-05, 2.69723477e-04, 2.17555719e-04, 5.40501879e-04,\n",
       "        2.98395664e-04, 8.86755065e-04, 6.99797658e-04, 4.66552378e-03,\n",
       "        1.80542663e-03, 3.27666605e-04, 5.35199520e-04, 2.26382397e-04,\n",
       "        2.92104442e-04, 9.18208211e-04, 1.76751373e-03, 5.16076378e-04,\n",
       "        1.82635392e-04, 5.85118812e-04, 2.01243750e-03, 8.20370617e-04,\n",
       "        3.13872517e-04, 6.06306024e-04, 4.60873822e-04, 4.02571532e-04,\n",
       "        3.67475335e-04, 1.77224869e-03, 3.68542118e-04, 2.53186881e-04,\n",
       "        2.59205977e-04, 1.18944159e-04, 3.48182150e-04, 2.40401247e-03,\n",
       "        9.57449339e-04, 1.75270356e-04, 1.37098818e-04, 3.76666635e-04,\n",
       "        4.26662235e-04, 1.74321707e-03, 2.66913815e-04, 9.34460578e-04,\n",
       "        2.17985177e-04, 1.47276666e-04, 2.12333579e-04, 2.19605436e-04,\n",
       "        2.31021644e-04, 1.26322020e-04, 1.11714411e-03, 4.13918341e-04,\n",
       "        2.57202065e-04, 4.84423715e-04, 3.88300956e-04, 5.15970138e-04,\n",
       "        2.84161268e-03, 1.08950832e-04, 1.78791070e-04, 2.86658438e-04,\n",
       "        1.55055948e-04, 2.83214194e-04, 2.05412438e-03, 2.45444599e-04,\n",
       "        9.12356192e-04, 7.46867881e-04, 4.02745330e-04, 1.73803579e-04,\n",
       "        2.02369634e-04, 8.51639174e-05, 1.72143986e-04, 6.14763619e-04,\n",
       "        5.17997917e-04, 2.53676965e-04, 1.48985250e-04, 7.28554009e-04,\n",
       "        5.46207642e-04, 7.21551752e-04, 1.29053889e-03, 6.46640644e-04,\n",
       "        8.60031581e-04, 1.49405417e-04, 1.28351915e-04, 3.71945650e-04,\n",
       "        3.25386298e-04, 2.25075473e-04, 8.46382216e-04, 2.52195247e-04,\n",
       "        6.86752794e-04, 3.67247368e-04, 1.82968455e-03, 1.99225247e-04,\n",
       "        4.54775785e-04, 3.59857772e-04, 1.77435777e-04, 3.33545339e-04,\n",
       "        2.68541776e-04, 3.11426491e-04, 3.06301744e-04, 3.31085157e-04,\n",
       "        1.58291738e-04, 1.42058665e-04, 1.86930785e-04, 6.18307960e-05,\n",
       "        3.26134900e-04, 8.11063983e-05, 2.11905274e-04, 7.45707955e-04,\n",
       "        2.79255667e-04, 4.04615298e-04, 1.44322669e-03, 2.89478873e-03,\n",
       "        1.82272074e-03, 1.89117875e-03, 9.62956917e-04, 6.64310280e-04,\n",
       "        3.25821808e-04, 3.47925826e-05, 1.41476516e-04, 6.87269583e-04,\n",
       "        2.17330032e-04, 3.14144742e-04, 2.31710479e-05, 8.31170111e-04,\n",
       "        1.19369837e-04, 6.57588541e-04, 1.77659274e-04, 1.90138863e-04,\n",
       "        3.73690284e-03, 1.27853959e-04, 1.37688441e-03, 8.01153543e-04,\n",
       "        1.31630890e-03, 3.57720368e-03, 1.60816286e-03, 4.84361914e-04,\n",
       "        1.09251217e-03, 4.08744316e-03, 2.45053772e-03, 3.01682720e-03,\n",
       "        7.01898681e-03, 5.07108301e-03, 1.83926359e-03, 8.09471063e-04,\n",
       "        8.31207235e-03, 2.89904718e-04, 1.42907097e-03, 1.97849461e-03,\n",
       "        3.56977934e-04, 4.90075160e-04, 2.75748048e-04, 4.12532999e-04,\n",
       "        4.80415209e-04, 1.37568980e-03, 1.29331177e-04, 2.33143383e-04,\n",
       "        1.99650152e-04, 4.72256223e-04, 2.20445454e-04, 3.23390644e-04,\n",
       "        2.24995571e-04, 1.59371886e-03, 3.18665382e-03, 3.22252902e-04,\n",
       "        1.47882725e-04, 2.19233370e-04, 3.42301784e-04, 2.77370687e-04,\n",
       "        2.21673764e-04, 2.22310821e-04, 8.37347678e-04, 4.12779950e-03,\n",
       "        4.76803125e-04, 2.99133575e-04, 2.21718952e-04, 1.16192898e-04,\n",
       "        1.89305997e-04, 8.54087099e-05, 6.55572672e-04, 2.46300374e-04,\n",
       "        1.07229577e-04, 7.53382188e-05, 1.51525119e-04, 5.81261828e-04,\n",
       "        2.59039933e-04, 3.41487010e-04, 1.29833521e-04, 1.49542367e-04,\n",
       "        3.06665591e-04, 2.57369807e-04, 1.28334731e-04, 5.10961175e-04,\n",
       "        3.13935708e-04, 1.70334309e-04, 6.61679900e-05, 1.90781945e-04,\n",
       "        1.94224561e-04, 2.60920936e-04, 1.45271663e-04, 3.85210873e-04,\n",
       "        8.71979721e-04, 1.83535359e-04, 4.02750891e-04, 8.00966887e-04,\n",
       "        5.75166428e-05, 1.97545346e-03, 1.81099049e-04, 1.90620367e-03,\n",
       "        1.01365576e-04, 3.54555334e-04, 2.49096436e-04, 2.87959019e-04,\n",
       "        2.60975089e-04, 1.43143815e-04, 3.10726221e-04, 1.95635650e-03,\n",
       "        2.99723262e-04, 3.23121304e-04, 1.89867575e-04, 1.42114402e-03,\n",
       "        5.46651588e-04, 1.72985585e-04, 6.35058666e-05, 4.17106784e-04,\n",
       "        1.65207273e-03, 3.62241938e-04, 4.96252552e-04, 3.70095476e-04,\n",
       "        1.85419035e-04, 1.77799488e-04, 2.08848234e-03, 1.98115975e-04,\n",
       "        1.38081836e-03, 1.81241311e-03, 6.92860002e-04, 1.27096685e-04,\n",
       "        4.33436804e-04, 2.04684004e-04, 4.76285410e-04, 3.48563768e-04,\n",
       "        3.21574136e-04, 5.20110182e-04, 3.50339383e-04, 1.32120594e-03,\n",
       "        2.66353596e-04, 5.24213818e-04, 2.76497898e-04, 3.50504180e-05,\n",
       "        3.34570949e-04, 1.89550142e-04, 3.24758781e-04, 3.65522851e-04,\n",
       "        3.54775350e-04, 2.84174104e-04, 5.57700382e-04, 1.72941245e-04,\n",
       "        5.46782921e-04, 2.92744759e-04, 2.97742773e-04, 4.85280585e-04,\n",
       "        4.48183494e-04, 1.29785069e-03, 2.09130786e-03, 3.02121984e-04,\n",
       "        1.93613597e-04, 9.65497652e-04, 1.63726633e-04, 2.60031444e-04,\n",
       "        1.23421725e-04, 3.84028602e-04, 2.81758005e-04, 2.01729813e-04,\n",
       "        9.11583203e-05, 9.62974500e-05, 3.55991530e-04, 1.65562717e-03,\n",
       "        4.39555993e-04, 1.09646836e-04, 4.56618003e-04, 1.47001820e-04,\n",
       "        1.51075207e-03, 4.52953598e-04, 1.86617848e-03, 1.27107418e-04,\n",
       "        1.02421758e-04, 1.31518431e-04, 1.10469555e-04, 9.40846236e-05,\n",
       "        2.23858343e-04, 1.61987738e-04, 7.01891793e-04, 2.47348014e-04,\n",
       "        2.35131705e-04, 4.01276173e-04, 9.22725612e-05, 1.02921017e-03,\n",
       "        8.24938529e-04, 3.65256317e-04, 1.91208097e-04, 3.94851803e-04,\n",
       "        1.19673056e-04, 4.37167027e-04, 1.81329792e-04, 1.50630937e-03,\n",
       "        3.11804616e-04, 1.19107093e-03, 1.92821122e-03, 2.31014607e-04,\n",
       "        1.30597007e-03, 1.46428034e-03, 2.76225371e-03, 1.53775762e-04,\n",
       "        2.05427314e-04, 1.29125742e-04, 4.02852480e-04, 2.85436197e-04,\n",
       "        2.05037538e-04, 3.08269584e-04, 6.74171524e-05, 3.82302995e-04,\n",
       "        1.88589705e-04, 4.17578401e-04, 1.77618315e-04, 5.38342850e-04,\n",
       "        1.01850787e-04, 2.72639934e-04, 3.50239785e-04, 1.90886897e-03,\n",
       "        2.16633718e-04, 1.67174750e-03, 6.86893154e-05, 2.01170889e-04,\n",
       "        2.09537675e-03, 2.11058145e-04, 4.63324571e-04, 2.13011810e-04,\n",
       "        1.04116714e-04, 1.10286260e-04, 1.19779144e-03]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.70707071, 0.72      , 0.70588235, 0.74747475, 0.72      ,\n",
       "        0.71287129, 0.74226804, 0.71428571, 0.74747475, 0.73469388,\n",
       "        0.7       , 0.70588235, 0.69473684, 0.71287129, 0.73267327,\n",
       "        0.72164948, 0.72916667, 0.76      , 0.72164948, 0.7184466 ,\n",
       "        0.69230769, 0.70103093, 0.72      , 0.74285714, 0.72164948,\n",
       "        0.74226804, 0.76470588, 0.72164948, 0.7254902 , 0.7047619 ,\n",
       "        0.6875    , 0.72      , 0.73786408, 0.70707071, 0.73267327,\n",
       "        0.75728155, 0.71428571, 0.7047619 , 0.69811321, 0.70103093,\n",
       "        0.74509804, 0.73786408, 0.71428571, 0.76      , 0.78      ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.70707071, 0.72      , 0.70588235, 0.74747475, 0.72      ,\n",
       "        0.71287129, 0.74226804, 0.71428571, 0.74747475, 0.73469388,\n",
       "        0.7       , 0.70588235, 0.69473684, 0.71287129, 0.73267327,\n",
       "        0.72164948, 0.72916667, 0.76      , 0.72164948, 0.7184466 ,\n",
       "        0.69230769, 0.70103093, 0.72      , 0.74285714, 0.72164948,\n",
       "        0.74226804, 0.76470588, 0.72164948, 0.7254902 , 0.7047619 ,\n",
       "        0.6875    , 0.72      , 0.73786408, 0.70707071, 0.73267327,\n",
       "        0.75728155, 0.71428571, 0.7047619 , 0.69811321, 0.70103093,\n",
       "        0.74509804, 0.73786408, 0.71428571, 0.76      , 0.78      ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.73267327, 0.7254902 , 0.68      , 0.72      , 0.71428571,\n",
       "        0.71287129, 0.70707071, 0.71428571, 0.71428571, 0.70707071,\n",
       "        0.72      , 0.69902913, 0.70707071, 0.69387755, 0.72727273,\n",
       "        0.70707071, 0.70103093, 0.72164948, 0.69387755, 0.69387755,\n",
       "        0.7184466 , 0.71428571, 0.72      , 0.73076923, 0.70833333,\n",
       "        0.72164948, 0.74747475, 0.68686869, 0.70588235, 0.71153846,\n",
       "        0.66666667, 0.7       , 0.73076923, 0.71428571, 0.72727273,\n",
       "        0.73267327, 0.71287129, 0.71153846, 0.7047619 , 0.68041237,\n",
       "        0.71287129, 0.72380952, 0.71428571, 0.72727273, 0.74      ,\n",
       "        0.72      , 0.73267327, 0.74      , 0.7       , 0.72727273,\n",
       "        0.72      , 0.70103093, 0.70707071, 0.71428571, 0.69387755,\n",
       "        0.72727273, 0.69306931, 0.7       , 0.72164948, 0.71428571,\n",
       "        0.69387755, 0.68686869, 0.71428571, 0.67326733, 0.7       ,\n",
       "        0.72380952, 0.7       , 0.70833333, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.70707071, 0.69306931, 0.72      , 0.70588235,\n",
       "        0.70707071, 0.72164948, 0.73267327, 0.69387755, 0.70707071,\n",
       "        0.74      , 0.67326733, 0.73786408, 0.71153846, 0.70707071,\n",
       "        0.70707071, 0.7254902 , 0.69387755, 0.69387755, 0.72      ,\n",
       "        0.66666667, 0.72      , 0.74509804, 0.68      , 0.72      ,\n",
       "        0.71428571, 0.69387755, 0.72      , 0.72727273, 0.67346939,\n",
       "        0.71428571, 0.72      , 0.67346939, 0.70707071, 0.71428571,\n",
       "        0.70707071, 0.68686869, 0.69387755, 0.67326733, 0.69387755,\n",
       "        0.68686869, 0.69306931, 0.72      , 0.71428571, 0.69387755,\n",
       "        0.69387755, 0.69387755, 0.67326733, 0.70707071, 0.72      ,\n",
       "        0.68686869, 0.71428571, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.70707071, 0.66      , 0.72      , 0.71153846, 0.7       ,\n",
       "        0.69387755, 0.72      , 0.69387755, 0.7       , 0.70707071,\n",
       "        0.66666667, 0.73267327, 0.72      , 0.65306122, 0.70707071,\n",
       "        0.70707071, 0.68686869, 0.72      , 0.72      , 0.65306122,\n",
       "        0.68686869, 0.71428571, 0.65306122, 0.70707071, 0.70707071,\n",
       "        0.66666667, 0.70707071, 0.70707071, 0.65306122, 0.67346939,\n",
       "        0.70707071, 0.65306122, 0.7       , 0.69387755, 0.67346939,\n",
       "        0.68686869, 0.68686869, 0.66666667, 0.69306931, 0.70707071,\n",
       "        0.66666667, 0.68686869, 0.70707071, 0.68686869, 0.68686869,\n",
       "        0.7       , 0.66666667, 0.7       , 0.71287129, 0.65306122,\n",
       "        0.68686869, 0.7       , 0.68686869, 0.69387755, 0.7       ,\n",
       "        0.60215054, 0.66666667, 0.66666667, 0.60215054, 0.66666667,\n",
       "        0.68686869, 0.60869565, 0.68      , 0.68686869, 0.63157895,\n",
       "        0.65306122, 0.66666667, 0.61052632, 0.65306122, 0.68      ,\n",
       "        0.63157895, 0.66666667, 0.70707071, 0.625     , 0.65306122,\n",
       "        0.68      , 0.63917526, 0.65306122, 0.69306931, 0.65979381,\n",
       "        0.68041237, 0.70707071, 0.63917526, 0.66      , 0.68627451,\n",
       "        0.63917526, 0.66666667, 0.68686869, 0.65979381, 0.69387755,\n",
       "        0.70707071, 0.63917526, 0.66      , 0.68627451, 0.63157895,\n",
       "        0.67346939, 0.68686869, 0.64583333, 0.69387755, 0.69387755]),\n",
       " 'split2_test_score': array([0.78095238, 0.75      , 0.74285714, 0.75      , 0.74285714,\n",
       "        0.74285714, 0.73786408, 0.76470588, 0.76190476, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.76190476, 0.75      , 0.75      ,\n",
       "        0.73786408, 0.76190476, 0.75471698, 0.74285714, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.75      , 0.74285714, 0.75      ,\n",
       "        0.74285714, 0.73394495, 0.72380952, 0.73584906, 0.73584906,\n",
       "        0.76923077, 0.75      , 0.74285714, 0.73786408, 0.75471698,\n",
       "        0.72727273, 0.73584906, 0.73584906, 0.72897196, 0.75471698,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75471698, 0.73394495,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.78095238, 0.75      , 0.74285714, 0.75      , 0.74285714,\n",
       "        0.74285714, 0.73786408, 0.76470588, 0.76190476, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.76190476, 0.75      , 0.75      ,\n",
       "        0.73786408, 0.76190476, 0.75471698, 0.74285714, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.75      , 0.74285714, 0.75      ,\n",
       "        0.74285714, 0.73394495, 0.72380952, 0.73584906, 0.73584906,\n",
       "        0.76923077, 0.75      , 0.74285714, 0.73786408, 0.75471698,\n",
       "        0.72727273, 0.73584906, 0.73584906, 0.72897196, 0.75471698,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75471698, 0.73394495,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.76190476, 0.74074074, 0.72897196, 0.75471698, 0.74285714,\n",
       "        0.74285714, 0.75471698, 0.74285714, 0.74285714, 0.74285714,\n",
       "        0.72897196, 0.72222222, 0.73584906, 0.75      , 0.73584906,\n",
       "        0.76190476, 0.74285714, 0.75471698, 0.74285714, 0.74074074,\n",
       "        0.72897196, 0.76190476, 0.74766355, 0.74285714, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.75471698, 0.73394495, 0.73394495,\n",
       "        0.76190476, 0.74285714, 0.73584906, 0.74285714, 0.73584906,\n",
       "        0.72727273, 0.75471698, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.73076923, 0.73584906, 0.72727273,\n",
       "        0.75247525, 0.75471698, 0.72897196, 0.73076923, 0.74074074,\n",
       "        0.75      , 0.75      , 0.73584906, 0.75      , 0.72380952,\n",
       "        0.73786408, 0.74074074, 0.73584906, 0.75471698, 0.74766355,\n",
       "        0.75      , 0.75      , 0.75471698, 0.73786408, 0.73394495,\n",
       "        0.73394495, 0.73584906, 0.76190476, 0.73584906, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.72380952, 0.73394495, 0.72222222,\n",
       "        0.73076923, 0.72897196, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.75471698, 0.71153846, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.74074074, 0.73584906, 0.72380952, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75925926, 0.75229358, 0.73267327, 0.74766355,\n",
       "        0.73584906, 0.74509804, 0.75      , 0.74285714, 0.75247525,\n",
       "        0.7254902 , 0.73584906, 0.73267327, 0.74285714, 0.75471698,\n",
       "        0.74509804, 0.75      , 0.76190476, 0.73267327, 0.74285714,\n",
       "        0.72897196, 0.7254902 , 0.75471698, 0.74766355, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.7184466 , 0.74545455, 0.73394495,\n",
       "        0.71698113, 0.75471698, 0.74285714, 0.73786408, 0.73584906,\n",
       "        0.74285714, 0.7047619 , 0.73873874, 0.73394495, 0.73076923,\n",
       "        0.74766355, 0.73584906, 0.73076923, 0.73584906, 0.74285714,\n",
       "        0.74      , 0.76923077, 0.74766355, 0.74      , 0.75471698,\n",
       "        0.75471698, 0.74      , 0.76190476, 0.75471698, 0.74509804,\n",
       "        0.75      , 0.76635514, 0.74509804, 0.75      , 0.74285714,\n",
       "        0.73267327, 0.75      , 0.76190476, 0.74509804, 0.73584906,\n",
       "        0.75925926, 0.74509804, 0.74285714, 0.75471698, 0.73786408,\n",
       "        0.74285714, 0.76190476, 0.73267327, 0.74285714, 0.74545455,\n",
       "        0.74509804, 0.73786408, 0.75471698, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.74509804, 0.75471698, 0.74545455, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.73076923, 0.74285714, 0.74285714,\n",
       "        0.70833333, 0.74      , 0.75247525, 0.70833333, 0.73267327,\n",
       "        0.74509804, 0.75510204, 0.74      , 0.76470588, 0.74      ,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.74509804, 0.74509804,\n",
       "        0.74747475, 0.73786408, 0.75      , 0.76      , 0.75      ,\n",
       "        0.75      , 0.75247525, 0.74509804, 0.73076923, 0.74      ,\n",
       "        0.73786408, 0.74285714, 0.76      , 0.75      , 0.72222222,\n",
       "        0.74      , 0.73786408, 0.73076923, 0.74509804, 0.73786408,\n",
       "        0.75      , 0.74      , 0.74285714, 0.72897196, 0.74      ,\n",
       "        0.73786408, 0.72380952, 0.74509804, 0.73076923, 0.75      ]),\n",
       " 'split3_test_score': array([0.78095238, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.7961165 , 0.78846154, 0.8       , 0.81553398, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.76190476, 0.80769231, 0.77669903, 0.76923077, 0.80769231,\n",
       "        0.80392157, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.8       , 0.76470588, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.77358491, 0.79245283, 0.77669903, 0.77669903, 0.8       ,\n",
       "        0.77669903, 0.76923077, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.78095238, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.7961165 , 0.78846154, 0.8       , 0.81553398, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.76190476, 0.80769231, 0.77669903, 0.76923077, 0.80769231,\n",
       "        0.80392157, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.8       , 0.76470588, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.77358491, 0.79245283, 0.77669903, 0.77669903, 0.8       ,\n",
       "        0.77669903, 0.76923077, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.81132075, 0.79245283, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.80769231, 0.81553398, 0.82242991,\n",
       "        0.8       , 0.79245283, 0.81132075, 0.8       , 0.79245283,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80769231, 0.78095238,\n",
       "        0.78095238, 0.82242991, 0.79245283, 0.79245283, 0.81904762,\n",
       "        0.81904762, 0.82352941, 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.78504673, 0.78504673, 0.81904762, 0.80769231,\n",
       "        0.80769231, 0.81904762, 0.78504673, 0.79245283, 0.81904762,\n",
       "        0.78504673, 0.76635514, 0.81904762, 0.7961165 , 0.80769231,\n",
       "        0.77669903, 0.79245283, 0.79245283, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.81132075, 0.7961165 , 0.80769231,\n",
       "        0.8       , 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.7961165 , 0.81904762, 0.82692308, 0.82692308, 0.79245283,\n",
       "        0.79245283, 0.82692308, 0.79245283, 0.78504673, 0.80769231,\n",
       "        0.81904762, 0.81904762, 0.81553398, 0.78846154, 0.77358491,\n",
       "        0.82692308, 0.77358491, 0.78504673, 0.81553398, 0.80769231,\n",
       "        0.81132075, 0.80769231, 0.79245283, 0.77777778, 0.80769231,\n",
       "        0.78504673, 0.78504673, 0.82692308, 0.81132075, 0.8       ,\n",
       "        0.77669903, 0.81132075, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.7961165 , 0.7961165 ,\n",
       "        0.8       , 0.78095238, 0.78431373, 0.8       , 0.78846154,\n",
       "        0.78431373, 0.80769231, 0.80769231, 0.80392157, 0.78095238,\n",
       "        0.78504673, 0.7961165 , 0.79245283, 0.78504673, 0.78846154,\n",
       "        0.8       , 0.81904762, 0.82692308, 0.78846154, 0.77358491,\n",
       "        0.80769231, 0.78846154, 0.78504673, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.78095238, 0.78095238, 0.80769231,\n",
       "        0.78504673, 0.79245283, 0.81553398, 0.81904762, 0.81132075,\n",
       "        0.78      , 0.76923077, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.78095238, 0.77669903,\n",
       "        0.80373832, 0.78095238, 0.77669903, 0.79245283, 0.79245283,\n",
       "        0.77669903, 0.79245283, 0.8       , 0.7961165 , 0.79245283,\n",
       "        0.79245283, 0.7961165 , 0.79245283, 0.79245283, 0.77669903,\n",
       "        0.79245283, 0.8       , 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.7961165 , 0.8       ,\n",
       "        0.81132075, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.79245283, 0.7962963 , 0.7961165 , 0.81132075, 0.8       ,\n",
       "        0.77419355, 0.78787879, 0.76470588, 0.77419355, 0.78787879,\n",
       "        0.76470588, 0.75789474, 0.78787879, 0.76470588, 0.77083333,\n",
       "        0.78      , 0.77669903, 0.7628866 , 0.78      , 0.77669903,\n",
       "        0.7755102 , 0.78      , 0.77669903, 0.82828283, 0.76470588,\n",
       "        0.78846154, 0.78787879, 0.76470588, 0.78846154, 0.7755102 ,\n",
       "        0.76      , 0.78846154, 0.79166667, 0.77227723, 0.8       ,\n",
       "        0.7628866 , 0.76470588, 0.79245283, 0.7628866 , 0.76      ,\n",
       "        0.8       , 0.8125    , 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.8       , 0.7628866 , 0.78      , 0.80769231]),\n",
       " 'split4_test_score': array([0.7628866 , 0.79591837, 0.78787879, 0.75510204, 0.75789474,\n",
       "        0.78787879, 0.71428571, 0.73469388, 0.7628866 , 0.76767677,\n",
       "        0.80412371, 0.79591837, 0.72164948, 0.7628866 , 0.78787879,\n",
       "        0.70103093, 0.74226804, 0.7628866 , 0.80808081, 0.82474227,\n",
       "        0.81632653, 0.77083333, 0.78350515, 0.76767677, 0.74226804,\n",
       "        0.7628866 , 0.7755102 , 0.80412371, 0.81632653, 0.7628866 ,\n",
       "        0.74226804, 0.78      , 0.7755102 , 0.7755102 , 0.75789474,\n",
       "        0.78787879, 0.80412371, 0.80412371, 0.7628866 , 0.7755102 ,\n",
       "        0.78787879, 0.76767677, 0.7755102 , 0.75      , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.7628866 , 0.79591837, 0.78787879, 0.75510204, 0.75789474,\n",
       "        0.78787879, 0.71428571, 0.73469388, 0.7628866 , 0.76767677,\n",
       "        0.80412371, 0.79591837, 0.72164948, 0.7628866 , 0.78787879,\n",
       "        0.70103093, 0.74226804, 0.7628866 , 0.80808081, 0.82474227,\n",
       "        0.81632653, 0.77083333, 0.78350515, 0.76767677, 0.74226804,\n",
       "        0.7628866 , 0.7755102 , 0.80412371, 0.81632653, 0.7628866 ,\n",
       "        0.74226804, 0.78      , 0.7755102 , 0.7755102 , 0.75789474,\n",
       "        0.78787879, 0.80412371, 0.80412371, 0.7628866 , 0.7755102 ,\n",
       "        0.78787879, 0.76767677, 0.7755102 , 0.75      , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.75510204, 0.82      , 0.8       , 0.73267327, 0.74      ,\n",
       "        0.7628866 , 0.73267327, 0.72727273, 0.74226804, 0.7755102 ,\n",
       "        0.80851064, 0.8       , 0.75247525, 0.7755102 , 0.75510204,\n",
       "        0.74      , 0.73684211, 0.72916667, 0.78      , 0.80412371,\n",
       "        0.78350515, 0.76      , 0.76      , 0.76767677, 0.75247525,\n",
       "        0.75789474, 0.75510204, 0.7755102 , 0.81632653, 0.77894737,\n",
       "        0.76      , 0.76      , 0.78      , 0.76470588, 0.7628866 ,\n",
       "        0.7755102 , 0.79591837, 0.79591837, 0.79166667, 0.76      ,\n",
       "        0.76767677, 0.76      , 0.74747475, 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.79207921, 0.80392157, 0.72727273, 0.76      ,\n",
       "        0.76      , 0.73469388, 0.72      , 0.72727273, 0.74226804,\n",
       "        0.79207921, 0.79166667, 0.72164948, 0.76      , 0.75510204,\n",
       "        0.74747475, 0.73267327, 0.74226804, 0.76470588, 0.79591837,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.76767677, 0.73469388,\n",
       "        0.75510204, 0.7755102 , 0.78431373, 0.78787879, 0.78350515,\n",
       "        0.79207921, 0.76767677, 0.76767677, 0.76767677, 0.76      ,\n",
       "        0.7755102 , 0.79207921, 0.79591837, 0.78350515, 0.79207921,\n",
       "        0.76767677, 0.77227723, 0.7755102 , 0.76767677, 0.7755102 ,\n",
       "        0.73684211, 0.78431373, 0.8       , 0.74226804, 0.77669903,\n",
       "        0.76470588, 0.73684211, 0.72      , 0.72727273, 0.72164948,\n",
       "        0.78      , 0.8125    , 0.72727273, 0.76470588, 0.74747475,\n",
       "        0.73469388, 0.7254902 , 0.72727273, 0.73469388, 0.78431373,\n",
       "        0.82474227, 0.72727273, 0.74509804, 0.75247525, 0.74226804,\n",
       "        0.74509804, 0.73469388, 0.76767677, 0.8       , 0.79591837,\n",
       "        0.74747475, 0.75247525, 0.7755102 , 0.7628866 , 0.74      ,\n",
       "        0.73469388, 0.76      , 0.78      , 0.80808081, 0.78      ,\n",
       "        0.75247525, 0.76767677, 0.74226804, 0.74      , 0.73469388,\n",
       "        0.74157303, 0.75510204, 0.77227723, 0.72527473, 0.74      ,\n",
       "        0.7961165 , 0.72340426, 0.73469388, 0.73267327, 0.73913043,\n",
       "        0.75247525, 0.77227723, 0.72340426, 0.73267327, 0.76470588,\n",
       "        0.71578947, 0.71287129, 0.7254902 , 0.73684211, 0.77669903,\n",
       "        0.78787879, 0.72916667, 0.76923077, 0.76470588, 0.72340426,\n",
       "        0.73267327, 0.75247525, 0.72916667, 0.76470588, 0.80808081,\n",
       "        0.73684211, 0.75      , 0.76470588, 0.71578947, 0.73267327,\n",
       "        0.74      , 0.75      , 0.76470588, 0.81188119, 0.75      ,\n",
       "        0.77669903, 0.78      , 0.73684211, 0.73267327, 0.74747475,\n",
       "        0.71264368, 0.73333333, 0.72916667, 0.68235294, 0.73333333,\n",
       "        0.72164948, 0.6744186 , 0.72340426, 0.72164948, 0.69565217,\n",
       "        0.71578947, 0.73469388, 0.70967742, 0.72916667, 0.73469388,\n",
       "        0.65934066, 0.73469388, 0.72727273, 0.7032967 , 0.75      ,\n",
       "        0.73469388, 0.70967742, 0.74226804, 0.72727273, 0.68131868,\n",
       "        0.73469388, 0.74747475, 0.7173913 , 0.74226804, 0.74747475,\n",
       "        0.72527473, 0.73684211, 0.75247525, 0.68131868, 0.73469388,\n",
       "        0.75510204, 0.7173913 , 0.74226804, 0.78431373, 0.7173913 ,\n",
       "        0.73684211, 0.76470588, 0.68888889, 0.70833333, 0.76      ]),\n",
       " 'mean_test_score': array([0.77270905, 0.78174977, 0.77606315, 0.77930295, 0.77659997,\n",
       "        0.77937332, 0.76912489, 0.77416567, 0.79063694, 0.77692065,\n",
       "        0.7719423 , 0.76755898, 0.76444581, 0.76888162, 0.77644644,\n",
       "        0.76056297, 0.77963493, 0.78568663, 0.78475881, 0.78062551,\n",
       "        0.77029827, 0.78254969, 0.77691462, 0.77151466, 0.78079255,\n",
       "        0.78293569, 0.77951481, 0.77763788, 0.78021576, 0.7605253 ,\n",
       "        0.77234878, 0.7754902 , 0.77442705, 0.77870438, 0.77915408,\n",
       "        0.77783105, 0.78021605, 0.76809626, 0.75333416, 0.77654865,\n",
       "        0.78138039, 0.7711448 , 0.78571124, 0.78246373, 0.7810718 ,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.77270905, 0.78174977, 0.77606315, 0.77930295, 0.77659997,\n",
       "        0.77937332, 0.76912489, 0.77416567, 0.79063694, 0.77692065,\n",
       "        0.7719423 , 0.76755898, 0.76444581, 0.76888162, 0.77644644,\n",
       "        0.76056297, 0.77963493, 0.78568663, 0.78475881, 0.78062551,\n",
       "        0.77029827, 0.78254969, 0.77691462, 0.77151466, 0.78079255,\n",
       "        0.78293569, 0.77951481, 0.77763788, 0.78021576, 0.7605253 ,\n",
       "        0.77234878, 0.7754902 , 0.77442705, 0.77870438, 0.77915408,\n",
       "        0.77783105, 0.78021605, 0.76809626, 0.75333416, 0.77654865,\n",
       "        0.78138039, 0.7711448 , 0.78571124, 0.78246373, 0.7810718 ,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.77690605, 0.78603378, 0.76828496, 0.77347805, 0.77187816,\n",
       "        0.77669004, 0.76745829, 0.76985015, 0.77441755, 0.77268039,\n",
       "        0.77753426, 0.76655036, 0.77057392, 0.77530612, 0.7735639 ,\n",
       "        0.77032385, 0.77076142, 0.77344419, 0.77187569, 0.76704567,\n",
       "        0.76315953, 0.78259786, 0.77265073, 0.77056072, 0.77441784,\n",
       "        0.77916358, 0.78174424, 0.76802709, 0.77654212, 0.76029993,\n",
       "        0.77407283, 0.76845456, 0.77720679, 0.77617927, 0.77981706,\n",
       "        0.77786047, 0.78680788, 0.77063174, 0.76375919, 0.77301245,\n",
       "        0.77283779, 0.77146017, 0.77318925, 0.77109236, 0.77391298,\n",
       "        0.7646251 , 0.78137475, 0.78068832, 0.7549482 , 0.77434219,\n",
       "        0.77676923, 0.76111222, 0.76960539, 0.77061191, 0.75823537,\n",
       "        0.78231699, 0.76890487, 0.75783634, 0.77419637, 0.77277461,\n",
       "        0.76219964, 0.7685917 , 0.77851255, 0.76525796, 0.77145352,\n",
       "        0.7778565 , 0.76434561, 0.77166069, 0.76654815, 0.75953145,\n",
       "        0.77263213, 0.78197406, 0.77257608, 0.77304735, 0.76242354,\n",
       "        0.78224223, 0.76700408, 0.77427823, 0.7685618 , 0.7749526 ,\n",
       "        0.78392864, 0.76491546, 0.7796689 , 0.76211552, 0.78248889,\n",
       "        0.77040402, 0.77628166, 0.77489786, 0.7731734 , 0.78091922,\n",
       "        0.74732092, 0.77968463, 0.78328785, 0.7566251 , 0.77766011,\n",
       "        0.77320946, 0.76065374, 0.76662745, 0.77178074, 0.75344801,\n",
       "        0.77094547, 0.78073407, 0.75154582, 0.77380053, 0.77021857,\n",
       "        0.75894115, 0.76263769, 0.76738024, 0.75044967, 0.76902761,\n",
       "        0.77542296, 0.75309563, 0.77332736, 0.76912502, 0.75560104,\n",
       "        0.76335684, 0.76571429, 0.75804707, 0.77682481, 0.7733171 ,\n",
       "        0.75814001, 0.77061535, 0.76908725, 0.7635709 , 0.7649558 ,\n",
       "        0.76835292, 0.75282748, 0.77256567, 0.77553077, 0.77624133,\n",
       "        0.76610965, 0.77119573, 0.76119564, 0.77205626, 0.76841927,\n",
       "        0.74964794, 0.76995325, 0.77116892, 0.74568053, 0.76517546,\n",
       "        0.78036843, 0.74626107, 0.76874097, 0.76854231, 0.74279775,\n",
       "        0.76495308, 0.77764788, 0.74126867, 0.76506681, 0.7722911 ,\n",
       "        0.73190104, 0.76277599, 0.76919016, 0.74622357, 0.76203069,\n",
       "        0.78188134, 0.74795379, 0.7695356 , 0.77369967, 0.73738939,\n",
       "        0.75567627, 0.77279876, 0.75058119, 0.76415541, 0.77693914,\n",
       "        0.75000962, 0.76524358, 0.77466307, 0.74509245, 0.76047982,\n",
       "        0.7713846 , 0.75331055, 0.77037208, 0.782829  , 0.75082582,\n",
       "        0.77007257, 0.77734099, 0.74930298, 0.76477319, 0.7711433 ,\n",
       "        0.71057533, 0.74807576, 0.74497913, 0.70621506, 0.74832094,\n",
       "        0.75166442, 0.70753681, 0.74952191, 0.75681676, 0.72080438,\n",
       "        0.74041012, 0.75506823, 0.72166857, 0.74228993, 0.75699516,\n",
       "        0.70500313, 0.73642224, 0.75691438, 0.7365074 , 0.74755342,\n",
       "        0.75696772, 0.73871091, 0.74429194, 0.75591456, 0.72186217,\n",
       "        0.74177774, 0.76350946, 0.73743612, 0.74652522, 0.7551943 ,\n",
       "        0.73091412, 0.74448105, 0.7605132 , 0.72199334, 0.74198813,\n",
       "        0.76243455, 0.73181331, 0.75002141, 0.76160435, 0.72689311,\n",
       "        0.74842972, 0.76073338, 0.72238753, 0.7417797 , 0.76865061]),\n",
       " 'std_test_score': array([0.04004678, 0.0454762 , 0.05235505, 0.03959942, 0.05343767,\n",
       "        0.04930868, 0.05262036, 0.05056826, 0.04396742, 0.0373489 ,\n",
       "        0.05103746, 0.0396085 , 0.05487725, 0.03917025, 0.03620764,\n",
       "        0.05322296, 0.04701571, 0.03441973, 0.04975545, 0.04842574,\n",
       "        0.05219392, 0.05606296, 0.0447355 , 0.03372441, 0.05825518,\n",
       "        0.04576475, 0.03306121, 0.04826513, 0.04320016, 0.03954192,\n",
       "        0.05839516, 0.04791444, 0.03475764, 0.05506955, 0.04278924,\n",
       "        0.03837268, 0.05007036, 0.04247152, 0.03593811, 0.04969119,\n",
       "        0.04047696, 0.03579344, 0.05339348, 0.03815429, 0.03211246,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.04004678, 0.0454762 , 0.05235505, 0.03959942, 0.05343767,\n",
       "        0.04930868, 0.05262036, 0.05056826, 0.04396742, 0.0373489 ,\n",
       "        0.05103746, 0.0396085 , 0.05487725, 0.03917025, 0.03620764,\n",
       "        0.05322296, 0.04701571, 0.03441973, 0.04975545, 0.04842574,\n",
       "        0.05219392, 0.05606296, 0.0447355 , 0.03372441, 0.05825518,\n",
       "        0.04576475, 0.03306121, 0.04826513, 0.04320016, 0.03954192,\n",
       "        0.05839516, 0.04791444, 0.03475764, 0.05506955, 0.04278924,\n",
       "        0.03837268, 0.05007036, 0.04247152, 0.03593811, 0.04969119,\n",
       "        0.04047696, 0.03579344, 0.05339348, 0.03815429, 0.03211246,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03468459, 0.04731971, 0.05669695, 0.05112019, 0.05627722,\n",
       "        0.05067905, 0.05101685, 0.05418655, 0.05325322, 0.04361997,\n",
       "        0.04450892, 0.04705291, 0.05086977, 0.05399647, 0.0474307 ,\n",
       "        0.04596631, 0.05849607, 0.04909612, 0.04950388, 0.04464262,\n",
       "        0.03334699, 0.0496852 , 0.04223026, 0.03221618, 0.05397928,\n",
       "        0.04964871, 0.03904538, 0.0470693 , 0.04964067, 0.03202918,\n",
       "        0.06598135, 0.05112595, 0.0444751 , 0.04686917, 0.051146  ,\n",
       "        0.04507871, 0.04858394, 0.04427648, 0.04180495, 0.06296765,\n",
       "        0.051426  , 0.05227151, 0.0540804 , 0.0423042 , 0.03825025,\n",
       "        0.03816129, 0.03518592, 0.04075258, 0.04914624, 0.04776831,\n",
       "        0.04941648, 0.04775401, 0.06341077, 0.05496931, 0.04958089,\n",
       "        0.04605519, 0.04897918, 0.04973751, 0.04176968, 0.04776757,\n",
       "        0.04459878, 0.06036219, 0.05310885, 0.05729076, 0.04813312,\n",
       "        0.04245504, 0.05436377, 0.0447545 , 0.04784204, 0.04860457,\n",
       "        0.05297003, 0.05570314, 0.0567386 , 0.04155167, 0.04368582,\n",
       "        0.05584914, 0.04322847, 0.03903681, 0.05219343, 0.05232356,\n",
       "        0.03616148, 0.06237612, 0.04188921, 0.03925779, 0.05371559,\n",
       "        0.0483467 , 0.04854556, 0.06032737, 0.05695128, 0.04513514,\n",
       "        0.04627783, 0.03721314, 0.0337867 , 0.05666147, 0.04448453,\n",
       "        0.05136025, 0.05026203, 0.04815016, 0.05320229, 0.05318021,\n",
       "        0.04538596, 0.04926359, 0.05646089, 0.05034035, 0.04468789,\n",
       "        0.04070753, 0.05619731, 0.05453729, 0.05031366, 0.04939743,\n",
       "        0.0605145 , 0.04866982, 0.0467345 , 0.04456927, 0.04236491,\n",
       "        0.04909074, 0.05427463, 0.05603018, 0.04673237, 0.04422357,\n",
       "        0.05431653, 0.04320362, 0.04282084, 0.04508954, 0.05897238,\n",
       "        0.05370971, 0.06357209, 0.0424579 , 0.0479248 , 0.05721302,\n",
       "        0.05175175, 0.04258794, 0.05030686, 0.06074379, 0.05184144,\n",
       "        0.05082291, 0.02993929, 0.03830383, 0.06216933, 0.04570842,\n",
       "        0.04792113, 0.04321683, 0.04409874, 0.04765355, 0.04998556,\n",
       "        0.04986715, 0.04487354, 0.05259948, 0.04792679, 0.04964042,\n",
       "        0.03950308, 0.0538643 , 0.05204479, 0.0531972 , 0.0538901 ,\n",
       "        0.05058668, 0.05720173, 0.04793628, 0.05494283, 0.03815762,\n",
       "        0.04771691, 0.05787219, 0.05644809, 0.04616912, 0.04741514,\n",
       "        0.05184756, 0.05616092, 0.0484463 , 0.04391026, 0.05364264,\n",
       "        0.05802794, 0.05058097, 0.04882764, 0.04886972, 0.05763106,\n",
       "        0.05447267, 0.05389363, 0.04186004, 0.05448764, 0.05680812,\n",
       "        0.0597256 , 0.05028869, 0.04754808, 0.06223935, 0.05289919,\n",
       "        0.05121671, 0.05800556, 0.04804207, 0.0533574 , 0.05196918,\n",
       "        0.0513209 , 0.05642011, 0.06001586, 0.05174424, 0.05536379,\n",
       "        0.05345758, 0.03865305, 0.04059513, 0.06837951, 0.05382093,\n",
       "        0.05107438, 0.05943977, 0.0527892 , 0.05202275, 0.04393994,\n",
       "        0.0376682 , 0.0427533 , 0.05519104, 0.04892661, 0.04920166,\n",
       "        0.0504471 , 0.04843146, 0.05236624, 0.0429899 , 0.0297756 ,\n",
       "        0.03491666, 0.056032  , 0.05486682, 0.04772114, 0.052434  ,\n",
       "        0.04923666, 0.05087227, 0.04760869, 0.0398153 , 0.04798134]),\n",
       " 'rank_test_score': array([301,  70, 199, 123, 185, 121, 393, 256,   3, 175, 318, 436, 482,\n",
       "        400, 192, 537, 115,  11,  15,  91, 369,  43, 177, 328,  87,  37,\n",
       "        117, 163, 102, 539, 310, 208, 243, 132, 126, 159, 100, 423, 596,\n",
       "        189,  73, 345,   9,  50,  78, 492, 113, 226, 194,  52, 107, 239,\n",
       "        151,  26, 434, 119, 250, 459, 332, 386, 447,  85,  21, 128,  83,\n",
       "         19, 235, 320,  39, 315,   5,  31, 149,  58, 170,  62,  64,  68,\n",
       "         95, 253,   1, 337, 136, 466, 153, 224, 201, 228, 109,  81, 564,\n",
       "        182,  24, 485, 231,  35, 417, 451, 259, 554, 216, 187, 560, 384,\n",
       "        335, 500, 281, 165, 534, 145, 134, 543, 271, 582, 407, 237, 284,\n",
       "        264,  76, 141, 471, 213, 279, 375, 233, 130, 343,  97, 143, 268,\n",
       "        168,  56, 356, 155,  45, 529, 274,  33, 603, 377, 218, 587, 352,\n",
       "        461, 572, 402,  13, 589, 241, 104, 591, 507, 382, 409, 247, 138,\n",
       "        566, 221, 277, 473, 339, 308, 324, 203,  47, 522,  66,  29, 457,\n",
       "        287, 354, 294, 349,  17, 455,  93, 147, 426, 261, 405, 641, 612,\n",
       "        505, 653, 619, 509, 661, 600, 580, 639, 527, 444, 637, 593, 494,\n",
       "        651, 548, 487, 520, 489, 421, 562, 430, 396, 644, 546, 313, 550,\n",
       "        440, 266, 432, 575, 359, 623, 515, 372, 389, 531, 428, 480, 503,\n",
       "        205, 608, 469, 211, 301,  70, 199, 123, 185, 121, 393, 256,   3,\n",
       "        175, 318, 436, 482, 400, 192, 537, 115,  11,  15,  91, 369,  43,\n",
       "        177, 328,  87,  37, 117, 163, 102, 539, 310, 208, 243, 132, 126,\n",
       "        159, 100, 423, 596, 189,  73, 345,   9,  50,  78, 492, 113, 226,\n",
       "        194,  52, 107, 239, 151,  26, 434, 119, 250, 459, 332, 386, 447,\n",
       "         85,  21, 128,  83,  19, 235, 320,  39, 315,   5,  31, 149,  58,\n",
       "        170,  62,  64,  68,  95, 253,   1, 337, 136, 466, 153, 224, 201,\n",
       "        228, 109,  81, 564, 182,  24, 485, 231,  35, 417, 451, 259, 554,\n",
       "        216, 187, 560, 384, 335, 500, 281, 165, 534, 145, 134, 543, 271,\n",
       "        582, 407, 237, 284, 264,  76, 141, 471, 213, 279, 375, 233, 130,\n",
       "        343,  97, 143, 268, 168,  56, 356, 155,  45, 529, 274,  33, 603,\n",
       "        377, 218, 587, 352, 461, 572, 402,  13, 589, 241, 104, 591, 507,\n",
       "        382, 409, 247, 138, 566, 221, 277, 473, 339, 308, 324, 203,  47,\n",
       "        522,  66,  29, 457, 287, 354, 294, 349,  17, 455,  93, 147, 426,\n",
       "        261, 405, 641, 612, 505, 653, 619, 509, 661, 600, 580, 639, 527,\n",
       "        444, 637, 593, 494, 651, 548, 487, 520, 489, 421, 562, 430, 396,\n",
       "        644, 546, 313, 550, 440, 266, 432, 575, 359, 623, 515, 372, 389,\n",
       "        531, 428, 480, 503, 205, 608, 469, 211, 179,   8, 420, 283, 322,\n",
       "        184, 438, 380, 246, 303, 167, 449, 364, 215, 276, 368, 358, 286,\n",
       "        323, 442, 502,  42, 304, 365, 245, 125,  72, 425, 191, 545, 258,\n",
       "        415, 173, 198, 106, 157,   7, 361, 496, 297, 298, 330, 292, 348,\n",
       "        263, 479,  75,  90, 586, 249, 181, 526, 381, 363, 556,  54, 399,\n",
       "        559, 255, 300, 517, 412, 140, 463, 331, 158, 484, 327, 450, 552,\n",
       "        305,  60, 306, 296, 514,  55, 443, 252, 413, 220,  23, 477, 112,\n",
       "        518,  49, 366, 196, 223, 293,  80, 628, 111,  28, 574, 161, 291,\n",
       "        536, 446, 326, 595, 351,  89, 606, 270, 371, 553, 512, 439, 611,\n",
       "        398, 210, 599, 289, 392, 579, 499, 454, 558, 180, 290, 557, 362,\n",
       "        395, 497, 475, 419, 602, 307, 207, 197, 453, 341, 525, 317, 416,\n",
       "        616, 379, 342, 632, 465,  99, 630, 404, 414, 643, 476, 162, 650,\n",
       "        468, 312, 663, 511, 391, 631, 519,  61, 626, 388, 273, 658, 578,\n",
       "        299, 610, 491, 174, 615, 464, 230, 633, 542, 334, 598, 367,  41,\n",
       "        607, 374, 172, 618, 478, 347, 672, 625, 634, 674, 622, 605, 673,\n",
       "        617, 571, 671, 655, 585, 670, 646, 568, 675, 660, 570, 659, 627,\n",
       "        569, 656, 636, 577, 669, 649, 498, 657, 629, 584, 665, 635, 541,\n",
       "        668, 647, 513, 664, 614, 524, 666, 621, 533, 667, 648, 411],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907208606102268"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775785\n",
      "F1: 0.652778\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a219cd6d8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.07, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
